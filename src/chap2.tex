\chapter{Methodology}

\section{Data, Model and Target Parameter}\label{data}

Other have proposed using estimators developed for lower dimensional ``causal
inference'' problems to derive nonparametric association estimators in context
of high-dimensional biomarker discovery studies \cite{tuglus2011targeted}. In
this case, the goals of analysis of more typical parametric approaches are
similar, but the approach is based on nonparametric estimands and can be
estimated with data-adaptive (machine learning) methods. Such data structures
typically consist of large matrices of biological expression values as well as
tables of phenotypic information on each subject. In particular, in later
sections, we will illustrate the use of our technique on data generated by the
\textit{Illumina Human Ref-8 BeadChips} platform, from a study which included
expression measures on $\sim 22,000$ genes as well as phenotypic information, on
a sample of 125 subjects. The aim of the analysis is to evaluate the association
of an environmental exposure (to benzene) on the expression measures of the
$\sim 22,000$ probes (genes) simultaneously, controlling for the several
aforementioned confounders. In our analysis, we considered three potential
confounding factors on the relationship of exposure and expression: age, sex,
and smoking status. This problem setup is easily generalizable to situations
with greater numbers of potential exposure biomarkers and confounders. For
instance, one  aim of analyzing data sets of the type described can be to rank
the importance of a set of candidate biomarkers based on their independent
associations with a treatment variable (exposure, in the case described in
~\ref{data}). In order to build a ranking of biomarkers, we start by defining a
variable importance measure (VIM) \cite{van2011targeted}.

Let $O = (W, A, Y) \sim P_{0}$ represent a random variable defined on the
observed data, where $W$ are the confounders, $A$ the exposure of interest,
and $Y=(Y_b, b = 1, \dots, B)$ a vector of potential biomarkers, with $P_{0}$
being the unknown probability distribution of the full data. For the specific
data set described in~\ref{data}, $W = (W_{1}, W_{2}, W_{3}, W_{4}, W_{5})$,
where age ($W_{1}$) is a continuous measure, gender ($W_{2}$) is binary,
smoking status ($W_{3}$) is binary, BMI ($W_{4}$) is a continuous measure, and
alcohol consumption ($W_{5}$) is binary; $A$ is a binary exposure; and $Y_{b}$
are miRNA expression measures.

To define the parameter of interest, generally, let $\Psi(P_{0})$ be the
target parameter based on a function $\Psi$ that maps the probability
distribution $P_{0}$ into the target feature of interest. Thus, the parameter
$\Psi(P_{0})$ is a function of the unknown probability distribution $P_{0}$,
defined on the full data. Let $P_{n}$ represent the  empirical distribution of
$(O_{1},O_{2}, \dots, O_{n})$. We will focus on cases when the $O_i$ are
i.i.d., but one can easily generalize the following when the data are clustered
(e.g., have repeated samples from same biological unit). We are interested in
substitution estimators of the form $\Psi(P_{n})$ --- that is, we apply the same
mapping as $\Psi$, but to the empirical distribution, $P_n$ to derive our
estimate (e.g, $\Psi$ could be the expectation operator). In using this general
definition, we expand the parameters of interest beyond coefficients in a
misspecified parametric statistical model, by defining a parameter as a feature
of the true probability distribution $P_{0}$ of the full data. Specifically, we
propose here what is referred to as a targeted variable importance
measure~\cite{bembom2009biomarker}:

\begin{eqnarray}
\label{targetparam}
\Psi_{b} \equiv \Psi_{b}(P_{0}) = \E _{W,0}[\E_{0}(Y_{b} \mid A = 1, W) - \E_{0}(Y_{b} \mid A = 0, W)].
\end{eqnarray}

This parameter (estimand) is generally referred to as the average treatment
effect, often denoted simply as the ATE~\cite{rosenbaum1983central}. It has
been shown that, under identifiability assumptions (e.g., no unmeasured
confounding), this parameter can be statistically estimated via targeted
maximum likelihood estimation~\cite{van2011targeted}. Such parameters are
significant in that they are not defined explicitly via parametric statistical
models, leaving one free to fit the requisite models data-adaptively, minimizing
assumptions wherever possible, and yet still estimating a relatively simple
parameter with rich scientific interpretation.

\subsection{Estimation}\label{estimation}

As noted previously (in Section~\ref{targetparam}), the target parameter is
defined as a feature of the unknown probability distribution $P_{0}$. While
there are several general classes of estimators available for estimating
$\Psi$, here we focus on a substitution estimator. Examining~\ref{targetparam},
one can anticipate that a substitution estimator will rely on estimates of
two components of the data-generating mechanism, $P_0$: $E_0 (Y \mid A=a, W)$
and $P_0(w)$, or the true regression of $Y$ on $(A,W)$ and the marginal
distribution of $W$. Let $Q^{b}_{0}(A, W) \equiv \E_{0}(Y_{b} \mid A, W)$, and
$Q^{b}_{n}(A,W)$ an estimate of this regression. If we use the empirical
distribution to estimate the joint marginal distribution of the $W$, then a
substitution estimator is:
\begin{equation}
\label{subest}
\Psi_b(P_{n}) = \frac{1}{n}\sum_{i = 1}^{n} Q^{b}_{n}(1, W_{i}) - Q^{b}_{n}(0, W_{i}).
\end{equation}
Below, we discuss recommendations for an initial estimate of $Q_0$ (Super
Learner), and a bias-reducing augmentation (targeted maximum likelihood
estimation; TMLE) with optimal properties for minimizing the error of
estimation and deriving robust inference.

\subsubsection{Using the Super Learner algorithm}\label{superlearner}

The first step in the two-stage TMLE procedure is to derive an initial estimate
of $Q^{b}_{0}$, referred to as $Q^{(b, 0)}_{n}$. For instance, one could assume
a parametric statistical model that results in~\ref{targetparam} being
equivalent to a regression coefficient (e.g., $Q^{b}_{0}(A, W) = \alpha^{b} +
\beta^{b}_{A}A + \beta^{b}_{W}W$). To avoid the pitfalls associated with model
misspecification, we elect to define~\ref{targetparam} in a nonparametric
statistical model, using data-adaptive tools to estimate $Q^{b}_{0}$.
Specifically, given that the true model $Q^{b}_{0}$ is typically unknown, more
accurate estimates may be derived by employing data-adaptive (machine learning)
algorithms in the estimation procedure.

This reliance on machine learning algorithms leads naturally to the issue of
choosing an optimal data-adaptive algorithm. To address this issue, we advocate
use of the Super Learner algorithm, which is a generalized stacking algorithm
for ensemble learning, implemented via cross-validation, which produces an
estimate that is optimally weighted to minimize the cross-validated risk. Using
this procedure, the predictions from a set of candidate algorithms are
combined, allowing for highly data-adaptive functional forms to be
specified~\cite{van2007super}.

Though the set of candidates algorithms in the library may be arbitrary, the
theoretical underpinnings of the Super Learner algorithm offer guidance as to
the type and number of learning algorithms that ought to be considered in the
fitting routine. In the rare case that one of the candidate learning algorithms
captured the true model and, consequently, converged to the correct estimate at
a parametric rate, the Super Learner algorithms has been shown to converge to
the same estimate at a near-parametric rate. As true relationships are rarely
captured by lone learning algorithms, the Super Learner will, up to a first
order term, do as well (in terms of risk) as an algorithm that chooses the
particular candidate learner based on full knowledge of the true distribution
(that is, a so-called Oracle Selector), a result that holds as long as the
number of candidate algorithms is polynomial in sample size. The Super Learner
algorithm is available as a software package~\cite{van2007super} for the R
programming language~\cite{R}.

\subsection{Targeted minimum loss-based estimation (TMLE)}\label{tmle}

While the SL estimate of $Q_0$ is performed to minimize the cross-validated
risk based on some appropriate loss-function, $Q_0$ is not the target of our
analysis, $\Psi_b$ is. There is no guarantee that, given a set of highly
data-adaptive learning algorithms (used by Super Learner), that the estimate of
$\Psi_b$ has a normal sampling distribution, especially when in cases of small
sample size. Fortunately,~\cite{van2011targeted} introduced an estimator of
$Q_0$ that not only ``targets'' the estimate of the regression towards the
particular parameter of interest but also ``smooths'' the estimator such that
the sampling distribution converges reliably to a normal distribution. This
``targeting'' step can be thought of as reducing bias, since the data-adaptive
selection procedure of Super Learner results in an estimate of $\Psi_b$ that
suffers from residual confounding. This form of confounding can occur, for
instance, if the variable selection step in the procedure estimating $Q^b_0$
leaves out any regressors that are, in truth, confounders of the association of
$A$ and $Y$. In this case, bias in estimation of $\Psi_b(P_0)$ is caused by
under-fitting. Thus, the resultant TMLE estimator is more robust to model
misspecification than the initial substitution estimator (based on the initial
fit of Super Learner), and is also (if one has consistent estimates of all
relevant portions of $P_0$) the semiparametrically locally efficient. For a
detailed discussion of the theory of TMLE and formal justifications of the
efficiency of the resultant estimator, consult the appendix of
~\cite{van2011targeted}.

Algorithmically, the TMLE-based estimator in our case is a simple
one-dimensional augmentation of the initial fit. Specifically, in the case of
a continuous outcome, following the initial Super Learner fit, one proceeds by
fitting a simple, one-dimensional regression:
$$
Q^{(b, 1)}_n(A, W) = Q^{(b, 0)}_n(A, W) + \hat{\epsilon} h_{\hat{g}}(A, W)
$$
where the initial fit, $Q^{(b, 0)}_n(A, W)$ is treated as an offset, and
$h_{\hat{g}}(A, W)$ is a ``clever'' covariate:
$$
h_{\hat{g}}(A, W) = \frac{I(A = 1)}{\hat{g}(1 \mid W)} - \frac{I(A = 0)}{\hat{g}(0 \mid W)}
$$
where $\hat{g}(1 \mid W)$ is an estimate of the $P(A = 1 \mid W)$ or the
propensity score~\cite{rosenbaum1983central}; $\hat{\epsilon}$ is the estimated
coefficient from the regression of $Y$ on $h_{\hat{g}}(A, W)$ treating
$Q^{(b, 0)}_n(A, W)$ (or its logit if regression is logistic) as the offset.
The selection of $\hat{g}$ can be made via a process that minimizes the
mean-squared error of the parameter of interest~\cite{gruber2010application},
but for our application, a simple main-terms logistic regression is usually
used. In the final step of this procedure, the TMLE estimate of $\Psi_b$ is
derived using the augmented estimate of $Q$:
\begin{eqnarray}
\hat{\Psi}_b(P^*_n) = \frac{1}{n}\sum^{n}_{i = 1}[Q^{(b, 1)}_n(1, W_i) - Q^{(b, 1)}_{n}(0, W_i)],
\end{eqnarray}
, where $P^*_n$ is the estimate of the data-generating distribution based on
TMLE, in this case, based on estimates $\hat{g},Q^{(b,1)}_n$.

\section{Statistical Inference}\label{inference}

\subsection{The approach of influence curves}\label{ic}

As shown in~\cite{van2011targeted}, $\Psi_b(P^*_n)$ is an asymptotically
linear estimator of $\Psi_b(P_0)$, with influence curve $IC(O_i)$ if it
satisfies
\begin{eqnarray}
\sqrt{n}(\Psi_b(P^*_n) - \Psi_b(P_0)) = \frac{1}{\sqrt{n}}\sum^{n}_{i = 1}IC(O_i) + o_p(1).
\label{eqn:IC}
\end{eqnarray}
Note from equation~\ref{eqn:IC} above that the variance of $\hat{\Psi}_b(P_n)$
is well approximated by the sample variance of the influence curve divided by
the sample size. When considering biomarkers, the plug-in influence curve (IC)
for the ATE is
\begin{eqnarray*}
IC_{b, n}(O_i) = \left[ \frac{I(A_i = 1)}{g_n(1 \mid W_i)} - \frac{I(A_i = 0)}
  {g_n(0 \mid W_i)} \right] (Y_{b, i} - Q^{(b, 1)}_{n}(A_i, W_i)) +
  Q^{(b, 1)}_{n}(1, W_i) - Q^{(b,1)}_{n}(0, W_i) - \Psi_b(P^*_n).
\end{eqnarray*}
With the above in hand, we can derive asymptotic p-values and confidence
intervals (CI) with a Wald-type approach:
\begin{eqnarray}
\mbox{p-value} = 2 \left[ 1 -
  \Phi(\frac{\vert\Psi_b(P^*_n)\vert}{\sigma^b_n/\sqrt{n}}) \right] \\
\mbox{(1 - $\alpha$) CI} = \Psi_b(P^*_n) \pm
  \frac{Z_{(1 - \alpha)} \sigma^b_{n}}{\sqrt{n}}
\label{eqn:tmleInference}
\end{eqnarray}
where $\sigma^b_{n}$ is the sample standard deviation of $IC_b$ and
$\Phi(\cdot)$ is the CDF of the standard normal distribution.

\subsection{The moderated t-statistic for influence curve-based
estimates}\label{modtIC}

In high-dimensional settings, with small sample sizes, direct application of
TMLE to obtaining joint inference for a targeted variable importance measure
can result in unstable standard error estimates, and thus erroneous
identification of biomarkers. This is particularly important if data-adaptive
procedures are being used, which can add to finite-sample non-robustness. To
address this problem, we apply the moderated t-statistic
of~\cite{smyth2005limma}, a technique that preserves accurate asymptotic
inference, yet, provide robust inference in finite (small) samples by drawing
on information across the many estimates of sampling variability (the
$\sigma^b_n$) using an empirical Bayes procedure. First developed for the
analysis of data from microarray experiments, the moderated t-statistic is
implemented in the immensely popular R package \textbf{Limma}, which provides a
suite of tools for analyzing the differential expression of genes using linear
models, borrowing information across all genes to provide stable and robust
inference for microarray data~\cite{smyth2005limma}. Previously, we noted that
a common way of making inference about the target parameter $\Psi_b$ is to
compute the influence curve-based values for $\Psi_b$, which can then be used
to calculate the corresponding standard errors of the influence curve of
$\Psi_b$. After obtaining these IC values, finding corresponding p-values and
making inference about $\Psi_b$ for each probe follow trivially.

The procedure for using the moderated t-statistic on IC-based estimates of
$\Psi_b$, using the \textbf{Limma} R package to impose variance shrinkage with
an empirical Bayes procedure, is as follows:

\begin{itemize}
\item Assume repeated tests, across $b$, of nulls and alternative:
    $H_0: \Psi_b(P_0) = 0, H_A: \Psi_b(P_0) \ne 0 $.
\item Find influence curve-based estimates for each probe, one at a time, using
    these to iteratively build a matrix of IC-based estimates of the ATE across
    all subjects, for all probes.
\item Since the IC-based estimates have mean zero, add in the corresponding
    estimates of $\Psi_b(P_n)$ to each row (probe/biomarker). This results in
    each row having an appropriate average ($\Psi_b(P_n)$) and sample variance
    equivalent to that of the influence curve for that probe ($\text{IC}_b$).
\item  Using the implementation readily available in the \textbf{Limma} R
    package, derive the moderated t-statistic ($ \tilde{t_b}, b = 1, \dots, B $)
    to the aforementioned matrix of IC-based estimates of the ATE, resulting in
    a multiple testing procedure across the relative to the null hypotheses
    listed above.
\item The resulting inference, based on the  shrinkage estimate of the
    sampling standard deviation of the influence curve ($\tilde{\sigma}^b_n$) is
    a weighted average of $\sigma^b_n$ and a value close to the average of all
    these sample standard deviation estimates across the biomarkers
    ($\overline{\sigma^b_n} \approx \frac{1}{B} \sum_{b = 1}^B \sigma^b_n$, or
    $\widetilde{\sigma}^b_n = wt_b \sigma^b_n+(1 - wt_b)\overline{\sigma^b_n}$,
    where $wt_b \in (0, 1)$). See~\cite{smyth2005limma} for a formal
    presentation. Asymptotically, as $n \rightarrow \infty$,
    $wt_b \rightarrow 1$, and thus
    $\widetilde{\sigma}^b_n \rightarrow \sigma^b_n$.
\item Use multiple testing corrections to obtain accurate simultaneous inference
    for all probes. In standard practice, we recommend the Benjamini-Hochberg
    procedure for controlling the False Discovery
    Rate~\cite{benjamini1995controlling}.
\end{itemize}

This procedure, as described above, will shrink (potentially) aberrant
estimates of variability estimates toward the center of the joint distribution,
with a particularly noticeable effect when the sample size is small. The
practical effect is that it tends to reduce the number of significant
biomarkers, driven by (potentially) erroneous underestimates of variation of
the parameter estimates of interest, $\Psi_b(P_n)$. The convenience of this
approach is that it can handle any asymptotically linear estimator (can be
represented as in (~\ref{eqn:IC})), which covers many if not most estimators of
parameters of interest. An open source R package, ``biotmle'', implementing the
described procedure, is available.
